
\vspace{-0.5\baselineskip}
\section{Related Work}
\label{sec:related}
\vspace{-0.3\baselineskip}

\textbf{~~~Structure from motion and single view geometry.}
As discussed in \secref{sec:intro}, geometry based methods, such as SFM~\cite{wu2011visualsfm}, ORB-SLAM~\cite{mur2015orb}, DTAM~\cite{NewcombeLD11}, rely on feature matching, which could be effective and efficient in many cases. 
However, they can fail at low texture, or drastic change of visual perspective \etc. More importantly, it can not extend to single view reconstruction where humans are good at.
Traditionally, specific rules are developed for single view geometry. Methods are dependent on either computing vanishing point~\cite{HoiemEH07}, following rules of BRDF~\cite{prados2006shape}, or abstract the scenes with major plane and box representations~\cite{DBLP:conf/iccv/SchwingFPU13,DBLP:conf/3dim/SrajerSPP14} \etc. Those methods can only obtain sparse geometry representations, and some of them require certain assumptions (\textit{e.g.} Lambertian, Manhattan world).

\textbf{Supervised single view geometry via CNN.}
With the advance of deep neural networks and their strong feature representation, dense geometry, i.e., pixel-wise depth and normal maps, can be readily estimated from a single image~\cite{wang2015designing,eigen2015predicting,laina2016deeper}. The learned CNN model show significant improvement comparing to other strategies based on hand-crafted features~\cite{karsch2014depth,ladicky2014pulling,zeisl2014discriminatively}. Others tried to improve the estimation further by appending a conditional random field (CRF)~\cite{DBLP:conf/cvpr/WangSLCPY15,Liu_2015_CVPR,li2015depth}. 
However, most works regard depth and normal predictions as independent tasks. \cite{peng2016depth} point out their correlations over large planar regions, and regularize the prediction using a dense CRF~\cite{kong2015intrinsic}, which improved the results on both depth and normal. However, all those methods require densely labeled ground truths, which are expensive to label in natural environments.

 % Long-range context and semantic cues are also incorporated in later works to refine the dense prediction by combining the networks with conditional random fields (CRF)~\cite{DBLP:conf/cvpr/WangSLCPY15, Liu_2015_CVPR, li2015depth, Wang_2015_CVPR}. Most recently, Eigen \textit{et al.}~\cite{DBLP:conf/iccv/EigenF15} further integrate depth and normal estimation into a large multi-scale network structure, which significantly improves the geometry estimation accuracy. Nevertheless, the output of the networks still lacks regularization over planar surfaces due to the adoption of pixel-wise loss functions during network training, resulting in unsatisfactory experience in 3D image editing applications.


%Later, the depth estimation from single-view input image is inherently an ill-posed problem which can only be solved with priors and semantic understanding of the scene - tasks that the convnets are good at. 
% There is a chain of works that propose learning depth/normal maps with deep convnets supervised with dense ground truth maps. \cite{liu2016learning} proposed to combine a convnet with a superpixel-based conditional random field (CRF) model, and to learn unary and pairwise terms for the depth map. Ladicky \etal \cite{ladicky2014pulling} incorporate semantics into their model to improve the per pixel depth estimation. Karsch \etal \cite{karsch2014depth} attempt to produce more consistent image level predictions by copying whole image depth from the training set, which requires the entire training set to be available during test time. Eigen \etal \cite{eigen2014depth,eigen2015predicting} showed it is possible to produce dense per pixel depth estimation through a two-scale deep network, trained on images and their corresponding ground truth depth maps. Many works have built upon this method using techniques like CRFs to improve accuracy \cite{li2015depth}, replacing regression loss with classification loss \cite{cao2017estimating}, implementing more robust loss function \cite{laina2016deeper}.

% Wang \etal \cite{wang2015designing} also built upon the basic idea of \cite{eigen2014depth} and incorporate scene geometrical priors to help learning, in the related problem of normal estimation. Data-driven normal estimation has not been long since the first approach, that directly tries to estimate surface normals from the data was proposed by Fouhey \etal \cite{fouhey2013data}.  Both this work and \cite{zeisl2014discriminatively} proposed by Ladicky \etal used hand-crafted features like texton, SIFT, local quantized ternary patterns. Li \etal \cite{\cite{li2015depth}} first proposed to estimate depth and normal jointly and showed performance gain. 

\textbf{Unsupervised single view geometry.}
Videos are easy to obtain at the present age, while hold much richer 3D information than single images. Thus, it attracts lots of interests if single view geometry can be learned through feature matching from videos. Recently, several deep learning methods have been proposed based on such an intuition. Deep3D \cite{xie2016deep3d} learns to generate the right view from the given left view by supervision of a stereo pair. In order to do back-propagation to depth values, it quantizes the depth space and trains to select the right one. 
% Video holds a great potential towards semantically meaningful visual representations. Recently, there have been a small number of deep network based methods for depth estimation. 
% DeepStereo \cite{flynn2016deepstereo} introduced a novel view synthesis network, while . This network generates new view images by selecting pixels from nearby images. The most appropriate depth values are selected to sample pixel values from neighboring images, based on plane sweep volume.
Concurrently, \cite{GargBR16} applied the similar supervision from stereo pairs, while the depth is kept continuous, They apply Taylor expansion to approximate the gradient for depth. \cite{godard2016unsupervised} extend Garg's work by including depth smoothness loss and left-right depth consistency. Most recently, \cite{zhou2017unsupervised} induces camera pose estimation into the training pipeline, which makes depth learning possible from monocular videos. However, they only focus on rigid scene without the ability to deal with moving objects. At the same time, \cite{kuznietsov2017semi} proposed a network to include modeling rigid object motion. Although vastly developed for depth estimation from video, normal information, which is also highly interesting for geometry prediction, has not been considered inside the pipeline. This paper fills in the missing part, and show that normal can serve as a natural regularization for depth estimation, which significantly improves the state-of-the-art performance. Finally, with our designed loss, we are able to  learn the indoor geometry where \cite{zhou2017unsupervised} usually fails to estimate.

% To deal with the problem of occlusion boundary caused by the rigid scene transformation and moving object, they proposed to mask out all these boundaries.

\section{Related Work}

\paragraph{Traditional methods for depth estimation}
Traditional methods, which rely heavily on the knowledge of underlying physical rules, are effective and efficient in many cases. But their reliance on accurate image correspondence can cause problems at areas of low texture, thin structure, drastic change of surface material, complex geometry, etc. 

\paragraph{Supervised learning for depth/normal estimation}
The depth estimation from single-view input image is inherently an ill-posed problem which can only be solved with priors and semantic understanding of the scene - tasks that the convnets are good at. There is a chain of works that propose learning depth/normal maps with deep convnets supervised with dense ground truth maps. Liu \etal \cite{liu2016learning} proposed to combine a convnet with a superpixel-based conditional random field (CRF) model, and to learn unary and pairwise terms for the depth map. Ladicky \etal \cite{ladicky2014pulling} incorporate semantics into their model to improve the per pixel depth estimation. Karsch \etal \cite{karsch2014depth} attempt to produce more consistent image level predictions by copying whole image depth from the training set, which requires the entire training set to be available during test time. Eigen \etal \cite{eigen2014depth,eigen2015predicting} showed it is possible to produce dense per pixel depth estimation through a two-scale deep network, trained on images and their corresponding ground truth depth maps. Many works have built upon this method using techniques like CRFs to improve accuracy \cite{li2015depth}, replacing regression loss with classification loss \cite{cao2017estimating}, implementing more robust loss function \cite{laina2016deeper}, 

Wang \etal \cite{wang2015designing} also built upon the basic idea of \cite{eigen2014depth} and incoporate scene geometrical priors to help learning, in the related problem of normal estimation. Data-driven normal estimation has not been long since the first approach, that directly tries to estimate surface normals from the data was proposed by Fouhey \etal \cite{fouhey2013data}.  Both this work and \cite{zeisl2014discriminatively} proposed by Ladicky \etal used hand-crafted features like texton, SIFT, local quantized ternary patterns. Li \etal \cite{\cite{li2015depth}} first proposed to estimate depth and normal jointly and showed performance gain. 


\paragraph{Unsupervised learning of depth from video}
Video holds a great potential towards semantically meaningful visual representations. Recently, there have been a small number of deep network based methods for depth estimation. Flynn \etal \cite{flynn2016deepstereo} introduced a novel view synthesis network. This network generates new view images by selecting pixels from nearby images. The most appropriate depth values are selected to sample pixel values from neighboring images, based on plane sweep planes. The Deep3D network proposed by Xie \etal \cite{xie2016deep3d} addressed the problem of generating right view image from an input of left view. Deep3D network produces a disparity map over all pixels using a reconstruction loss.

Garg \etal \cite{GargBR16} applied the novel view synthesis techniques to the problem of depth estimation. They trained a network for monocular depth estimation. During training, the network takes stereo image pairs as input, uses image reconstruction loss between left and right view as self-supervision. Godard \etal \cite{godard2016unsupervised} extend Garg's work by including depth smoothness loss and left-right depth consistency. The training data for both methods need to be stereo image pairs. Zhou \etal \cite{zhou2017unsupervised} tries to remove the stereo image pair constraint for training data. They proposed a network to predict the depth map and camera motion. To deal with the problem of occlusion boundary caused by the rigid scene transformation and moving object, they proposed to mask out all these boundaries. Concurrently, Vijayanarasimhan \etal \cite{kuznietsov2017semi} proposed a network for joint depth estimation, camera motion and scene motion. Their work also explored different variants by incorporating supervision. 
\section{Experiments}
\label{sec:experiments}

In this section, we introduce the training details, datasets, evaluation metrics. An ablation study of how much each component of the framework contributes and a performance comparison with other supervised or unsupervised methods are also present in this section.

\subsection{Implementation details.}

Our framework is implemented with publicly available TensorfFlow \cite{abadi2016tensorflow} platform and has 34 million trainable variables in total. During training, batch normalization \cite{ioffe2015batch} is used for all layers except for output layer. Adam optimizer is implemented with parameters $\beta_1 = 0.9$, $\beta_2=0.000$, $\epsilon=10^{-8}$. Learning rate,and batch size are set to be $2\times10^{-3}$ and 4 respectively.

The length of input sequence is fixed to be 3 and the input frames are resized to $128 \times 416$. The middle frame is treated as target image and the other two are source images. With the settings above, the network starts to show meaningful results after 3 epochs of training, and converge at the end of 5th epoch. On a Nvidia Titan X (Pascal) GPU, the training process takes around 6 hours. The number of epochs and absolute time needed for convergence is much less than \cite{godard2016unsupervised} (50 epochs, 25 hours) and \cite{zhou2017unsupervised} (15 epochs).

As the predicted depth is not absolute value but defined up to a scale, we correct the scale factor by enforcing the predicted median matching the ground truth median. That is, multiplying the predicted depth by a factor: $\hat{f} = median(D_{gt}) / median(D_{pred})$. 

\subsection{Datasets and metrics}
\textbf{Training.}
Theorectically, our framework can be trained on frame sequences captured with a monocular camera. To better compare with other methods, we train the framework on popular KITTI 2015 \cite{geiger2012we} dataset. KITTI 2015 dataset is a large dataset suite for multiple tasks, including optical flow, 3D object detection and tracking, semantic segmenations, etc. All RGB and gray-scale videos are captured by stereo cameras from 61 scenes, with a typical image being $1242 \times 375$ in original size.

Raw videos captured by both left and right cameras are downloaded, which are treated independently for training. The same training frame sequences as in \cite{zhou2017unsupervised} are used for training: training split used by Eigen \etal  \cite{eigen2014depth} excluding frames from test scenes and static sequences. This results in a total of 40,109 trainig sequences and 4431 validation sequences. Different from \cite{godard2016unsupervised}, no other data augmentation has been performed.

\textbf{Testing.} 
There are two splits of KITTI 2015 test data: (1) KITTI split contains 200 high-quality disparity images provided as part of official KITTI training set; (2) Eigen split contains 697 test images proposed by \cite{eigen2014depth}. To better compare with other unsupervised and supervised methods, we present evaluation methods on both two splits. 

The depth ground truth of KITTI split contains sparse depth map with CAD models in place of moving cars. It provides better quality dpeth than projected Velodyne laser scanned points but has ambiguous depth value on object boundaries where the CAD model doesn't align with the images. The predicted depth is capped at 80 meters as in \cite{godard2016unsupervised} and \cite{zhou2017unsupervised}. The depth ground truth of Eigen split is generated by projecting 3D points scanned from Velodyne laser to the camera view. This produces depth values for less than 5\% of all pixels in the RGB images. To be consistent when comparing with other methods, the same crop as in \cite{eigen2014depth} is implemented when testing.

The normal ground truth for two splits is generated by applying depth2normal \ref{chap:d2n} layer on inpainted depth ground truth. The inpainting algorithm by \cite{silberman2012indoor} has been used. The inpainting depth and normal results are shown in later sections for visualization. However, for evaluation, only the sparse points with depth ground truth are used.

\textbf{Metrics.} The same depth evaluation and normal evaluation metrics as in \cite{eigen2014depth} and \cite{eigen2015predicting} have been implemented.

Abs Rel: $\frac{1}{|D|}\sum_{d_{pred}\in D}|d_gt - d_{pred}|/d_gt$

Sq Rel: $\frac{1}{|D|}\sum_{d_{pred}\in D}||d_gt - d_{pred}||^2/d_gt$

RMSE: $\sqrt{\frac{1}{|D|}\sum_{d_{pred}\in D}||d_{gt} - d_{pred}||^2}$

RMSE log: $\sqrt{\frac{1}{|D|}\sum_{d_{pred}\in D}||\log d_{gt} - \log d_{pred}||^2}$

$\delta<thr$: $\%$ of $d_{pred}\in D$, s.t. $max(\frac{d_gt}{d_{pred}}, \frac{d_{pred}}{d_{gt}})<thr$

mean: $\frac{1}{|N|}\sum_{n_{pred}\in N}(n_{gt}\cdot n_{pred})$

median: $median([(n_{gt}\cdot n_{pred})]_{n_{pred} \in N})$

degree: $\%$ of $n_{pred} \in N$, s.t. $(n_{gt}\cdot n_{pred}) < degree$

\subsection{Ablation study}

An ablation study is conducted to investigate how much each component contributes to the final performance, evaluated on the KITTI split.

\textbf{Depth and normal geometry consistency.} The impact of adding depth-normal consistency is explored by removing normal2depth layer in the framework. The inverse warping process introduced in section \ref{chap:warping} takes input image and directly predicted depth map as input. The performance of framework trained without normal2depth layer is shown as the row ``Ours (no d-n)" in Table \ref{tbl:ablation}. Besides the performance gain after adding depth and normal consistency, the network converges considerably faster compared to without leveraging such consistency: the full network converges after 5 epochs of training and the network without such consistency regularization converges at 13th epoch.

\textbf{Image gradient in smoothness term.} We explore the effectivness of adding image gradient into smoothness term. By setting $\alpha=0$ in the edge-aware smoothness loss, the image gradient has no effect on the weight of smoothness loss. The results of this variant is shown as ``Ours (smooth no gradient)" in Table \ref{tbl:ablation}.

\textbf{Image gradient in normal2depth layer.} When setting the $\alpha =0$ in normal2depth layer, the normal direction contributes equally to each ``shifted" depth map. With image gradient in normal2depth layer, the normal direction will only contribute to those neighboring points $q$ where there is small image gradient between central point $p$ and $q$, \ie the two points most likely lie on the same plane. With such constraint, the depth evaluation performance is better. 

\textbf{Normal smoothness.} We explore the impact of normal smoothness term by evaluating normal performance comparing framework with and without normal smooothness loss term. The visualization results are shown in Figure \ref{fig:normalsmooth}. When trained with normal smoothness term, the network generates more reasonable results both qualitatively and quantitatively.

\begin{table*}[]
\centering
\caption{Ablation study of the impact of each component of the framework.}
\label{tbl:ablation}
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{Methods}  & \multicolumn{4}{c|}{Lower the better} & \multicolumn{3}{c|}{Higher the better}                  \\ \cline{2-8} 
                          & Abs Rel  & Sq Rel  & RMSE  & RMSE log & $\delta < 1.25$ & $\delta < 1.25^2$ & $\delta < 1.25^3$ \\ \hline
Ours (no d-n)             & 0.208    & 2.286   & 7.462 & 0.297    & 0.693           & 0.875             & 0.948             \\ \hline
Ours (smooth no gradient) & 0.189    & 1.627   & 7.017 & 0.280    & 0.713           & 0.891             & 0.957             \\ \hline
Ours (n2d no img grad)    & 0.179    & 1.566   & 7.247 & 0.272    & 0.720           & 0.895             & 0.959             \\ \hline
Ours (no normal smooth)   & 0.172    & 1.559   & 6.794 & 0.252    & 0.744           & 0.910             & 0.969             \\ \hline
\end{tabular}
\end{table*} 

%\textbf{Image gradient matching.} There is no obvious 

\subsection{Comparison with other methods}

To compare with other supervised and unsupervised methods, our framework is evaluated on both KITTI and Eigen split. The depth evaluation results are shown in Table \ref{tbl:res_kitti} and Table \ref{tbl:eigen}. Our method outperforms some  unsupervised methods \cite{zhou2017unsupervised} and \cite{kuznietsov2017semi} and even outperforms some supervised methods \cite{eigen2014depth} and \cite{liu2016learning}. 

Our performance is prior to two methods, \cite{godard2016unsupervised} and \cite{kuznietsov2017semi} (supervised, semi-supervised). It is worth noting that \cite{kuznietsov2017semi} (supervised, semi-supervised) utilizes the depth ground truth and \cite{godard2016unsupervised} takes stereo image pairs as input, which implies the camera motion between two images are known. On both two test splits, our method outperforms \cite{godard2016unsupervised} on the Sq Rel metric. As Sq Rel metric penalizes large depth difference, our method generates depth maps without much outlier depths.

To our knowledge, there has not been works that report normal performance on KITTI 2015 dataset. We thus compare the our normal performance with normals generated by applying depth2normal layer on depth maps of two publicly available unsupervised methods \cite{zhou2017unsupervised} and \cite{godard2016unsupervised}. As shown in Table \ref{tbl:normal}, our method outperforms both methods under all metrics.